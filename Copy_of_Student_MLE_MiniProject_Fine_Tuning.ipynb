{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ntavakoulnia/ntavakoulnia/blob/main/Copy_of_Student_MLE_MiniProject_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyXucUekO19i"
      },
      "source": [
        "# Mini Project: Transfer Learning with Keras\n",
        "\n",
        "Transfer learning is a machine learning technique where a model trained on one task is used as a starting point to solve a different but related task. Instead of training a model from scratch, transfer learning leverages the knowledge learned from the source task and applies it to the target task. This approach is especially useful when the target task has limited data or computational resources.\n",
        "\n",
        "In transfer learning, the pre-trained model, also known as the \"base model\" or \"source model,\" is typically trained on a large dataset and a more general problem (e.g., image classification on ImageNet, a vast dataset with millions of labeled images). The knowledge learned by the base model in the form of feature representations and weights captures common patterns and features in the data.\n",
        "\n",
        "To perform transfer learning, the following steps are commonly followed:\n",
        "\n",
        "1. Pre-training: The base model is trained on a source task using a large dataset, which can take a considerable amount of time and computational resources.\n",
        "\n",
        "2. Feature Extraction: After pre-training, the base model is used as a feature extractor. The last few layers (classifier layers) of the model are discarded, and the remaining layers (feature extraction layers) are retained. These layers serve as feature extractors, producing meaningful representations of the data.\n",
        "\n",
        "3. Fine-tuning: The feature extraction layers and sometimes some of the earlier layers are connected to a new set of layers, often called the \"classifier layers\" or \"task-specific layers.\" These layers are randomly initialized, and the model is trained on the target task with a smaller dataset. The weights of the base model can be frozen during fine-tuning, or they can be allowed to be updated with a lower learning rate to fine-tune the model for the target task.\n",
        "\n",
        "Transfer learning has several benefits:\n",
        "\n",
        "1. Reduced training time and resource requirements: Since the base model has already learned generic features, transfer learning can save time and resources compared to training a model from scratch.\n",
        "\n",
        "2. Improved generalization: Transfer learning helps the model generalize better to the target task, especially when the target dataset is small and dissimilar from the source dataset.\n",
        "\n",
        "3. Better performance: By starting from a model that is already trained on a large dataset, transfer learning can lead to better performance on the target task, especially in scenarios with limited data.\n",
        "\n",
        "4. Effective feature extraction: The feature extraction layers of the pre-trained model can serve as powerful feature extractors for different tasks, even when the task domains differ.\n",
        "\n",
        "Transfer learning is commonly used in various domains, including computer vision, natural language processing (NLP), and speech recognition, where pre-trained models are fine-tuned for specific applications like object detection, sentiment analysis, or speech-to-text.\n",
        "\n",
        "In this mini-project you will perform fine-tuning using Keras with a pre-trained VGG16 model on the CIFAR-10 dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYuE9O6I2uRY"
      },
      "source": [
        "First, import all the libraries you'll need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kLWR1DfQPakn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEB60YsX2yUf"
      },
      "source": [
        "The CIFAR-10 dataset is a widely used benchmark dataset in the field of computer vision and machine learning. It stands for the \"Canadian Institute for Advanced Research 10\" dataset. CIFAR-10 was created by researchers at the CIFAR institute and was originally introduced as part of the Neural Information Processing Systems (NIPS) 2009 competition.\n",
        "\n",
        "The dataset consists of 60,000 color images, each of size 32x32 pixels, belonging to ten different classes. Each class contains 6,000 images. The ten classes in CIFAR-10 are:\n",
        "\n",
        "1. Airplane\n",
        "2. Automobile\n",
        "3. Bird\n",
        "4. Cat\n",
        "5. Deer\n",
        "6. Dog\n",
        "7. Frog\n",
        "8. Horse\n",
        "9. Ship\n",
        "10. Truck\n",
        "\n",
        "The images are evenly distributed across the classes, making CIFAR-10 a balanced dataset. The dataset is divided into two sets: a training set and a test set. The training set contains 50,000 images, while the test set contains the remaining 10,000 images.\n",
        "\n",
        "CIFAR-10 is often used for tasks such as image classification, object recognition, and transfer learning experiments. The relatively small size of the images and the variety of classes make it a challenging dataset for training machine learning models, especially deep neural networks. It also serves as a good dataset for teaching and learning purposes due to its manageable size and straightforward class labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp0MVIbiFonL"
      },
      "source": [
        "Here are your tasks:\n",
        "\n",
        "1. Load the CIFAR-10 dataset after referencing the documentation [here](https://keras.io/api/datasets/cifar10/).\n",
        "2. Normalize the pixel values so they're all in the range [0, 1].\n",
        "3. Apply One Hot Encoding to the train and test labels using the [to_categorical](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical) function.\n",
        "4. Further split the the training data into training and validation sets using [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). Use only 10% of the data for validation.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ndNbwjaaSvs-"
      },
      "outputs": [],
      "source": [
        "# Load the CIFAR-10 dataset\n",
        "# The data for the CIFAR-10 is loaded using the load_data technique training and testing datasets.\n",
        "# The CIFAR-10 has 60,000 images according to the documentation, where 50000 images are used for training and 10000 are used for testing.\n",
        "# The x_train and x_test illustrate the array that contains the RGB images of shape (32,32,3), representing 32x32 pixels with a 3 color channel (red,green,and blue).\n",
        "# The y_train and y_test indicate arrays for classe labels, one for each image. For example class 0 can represent a bus for instance, and class 1 can represent a house.\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8H6KUgh2SxxR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c6573eb-c727-4b89-9f17-67568c378cba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[0.23137255, 0.24313726, 0.24705882],\n",
              "         [0.16862746, 0.18039216, 0.1764706 ],\n",
              "         [0.19607843, 0.1882353 , 0.16862746],\n",
              "         ...,\n",
              "         [0.61960787, 0.5176471 , 0.42352942],\n",
              "         [0.59607846, 0.49019608, 0.4       ],\n",
              "         [0.5803922 , 0.4862745 , 0.40392157]],\n",
              "\n",
              "        [[0.0627451 , 0.07843138, 0.07843138],\n",
              "         [0.        , 0.        , 0.        ],\n",
              "         [0.07058824, 0.03137255, 0.        ],\n",
              "         ...,\n",
              "         [0.48235294, 0.34509805, 0.21568628],\n",
              "         [0.46666667, 0.3254902 , 0.19607843],\n",
              "         [0.47843137, 0.34117648, 0.22352941]],\n",
              "\n",
              "        [[0.09803922, 0.09411765, 0.08235294],\n",
              "         [0.0627451 , 0.02745098, 0.        ],\n",
              "         [0.19215687, 0.10588235, 0.03137255],\n",
              "         ...,\n",
              "         [0.4627451 , 0.32941177, 0.19607843],\n",
              "         [0.47058824, 0.32941177, 0.19607843],\n",
              "         [0.42745098, 0.28627452, 0.16470589]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.8156863 , 0.6666667 , 0.3764706 ],\n",
              "         [0.7882353 , 0.6       , 0.13333334],\n",
              "         [0.7764706 , 0.6313726 , 0.10196079],\n",
              "         ...,\n",
              "         [0.627451  , 0.52156866, 0.27450982],\n",
              "         [0.21960784, 0.12156863, 0.02745098],\n",
              "         [0.20784314, 0.13333334, 0.07843138]],\n",
              "\n",
              "        [[0.7058824 , 0.54509807, 0.3764706 ],\n",
              "         [0.6784314 , 0.48235294, 0.16470589],\n",
              "         [0.7294118 , 0.5647059 , 0.11764706],\n",
              "         ...,\n",
              "         [0.72156864, 0.5803922 , 0.36862746],\n",
              "         [0.38039216, 0.24313726, 0.13333334],\n",
              "         [0.3254902 , 0.20784314, 0.13333334]],\n",
              "\n",
              "        [[0.69411767, 0.5647059 , 0.45490196],\n",
              "         [0.65882355, 0.5058824 , 0.36862746],\n",
              "         [0.7019608 , 0.5568628 , 0.34117648],\n",
              "         ...,\n",
              "         [0.84705883, 0.72156864, 0.54901963],\n",
              "         [0.5921569 , 0.4627451 , 0.32941177],\n",
              "         [0.48235294, 0.36078432, 0.28235295]]],\n",
              "\n",
              "\n",
              "       [[[0.6039216 , 0.69411767, 0.73333335],\n",
              "         [0.49411765, 0.5372549 , 0.53333336],\n",
              "         [0.4117647 , 0.40784314, 0.37254903],\n",
              "         ...,\n",
              "         [0.35686275, 0.37254903, 0.2784314 ],\n",
              "         [0.34117648, 0.3529412 , 0.2784314 ],\n",
              "         [0.30980393, 0.31764707, 0.27450982]],\n",
              "\n",
              "        [[0.54901963, 0.627451  , 0.6627451 ],\n",
              "         [0.5686275 , 0.6       , 0.6039216 ],\n",
              "         [0.49019608, 0.49019608, 0.4627451 ],\n",
              "         ...,\n",
              "         [0.3764706 , 0.3882353 , 0.30588236],\n",
              "         [0.3019608 , 0.3137255 , 0.24313726],\n",
              "         [0.2784314 , 0.28627452, 0.23921569]],\n",
              "\n",
              "        [[0.54901963, 0.60784316, 0.6431373 ],\n",
              "         [0.54509807, 0.57254905, 0.58431375],\n",
              "         [0.4509804 , 0.4509804 , 0.4392157 ],\n",
              "         ...,\n",
              "         [0.30980393, 0.32156864, 0.2509804 ],\n",
              "         [0.26666668, 0.27450982, 0.21568628],\n",
              "         [0.2627451 , 0.27058825, 0.21568628]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.6862745 , 0.654902  , 0.6509804 ],\n",
              "         [0.6117647 , 0.6039216 , 0.627451  ],\n",
              "         [0.6039216 , 0.627451  , 0.6666667 ],\n",
              "         ...,\n",
              "         [0.16470589, 0.13333334, 0.14117648],\n",
              "         [0.23921569, 0.20784314, 0.22352941],\n",
              "         [0.3647059 , 0.3254902 , 0.35686275]],\n",
              "\n",
              "        [[0.64705884, 0.6039216 , 0.5019608 ],\n",
              "         [0.6117647 , 0.59607846, 0.50980395],\n",
              "         [0.62352943, 0.6313726 , 0.5568628 ],\n",
              "         ...,\n",
              "         [0.40392157, 0.3647059 , 0.3764706 ],\n",
              "         [0.48235294, 0.44705883, 0.47058824],\n",
              "         [0.5137255 , 0.4745098 , 0.5137255 ]],\n",
              "\n",
              "        [[0.6392157 , 0.5803922 , 0.47058824],\n",
              "         [0.61960787, 0.5803922 , 0.47843137],\n",
              "         [0.6392157 , 0.6117647 , 0.52156866],\n",
              "         ...,\n",
              "         [0.56078434, 0.52156866, 0.54509807],\n",
              "         [0.56078434, 0.5254902 , 0.5568628 ],\n",
              "         [0.56078434, 0.52156866, 0.5647059 ]]],\n",
              "\n",
              "\n",
              "       [[[1.        , 1.        , 1.        ],\n",
              "         [0.99215686, 0.99215686, 0.99215686],\n",
              "         [0.99215686, 0.99215686, 0.99215686],\n",
              "         ...,\n",
              "         [0.99215686, 0.99215686, 0.99215686],\n",
              "         [0.99215686, 0.99215686, 0.99215686],\n",
              "         [0.99215686, 0.99215686, 0.99215686]],\n",
              "\n",
              "        [[1.        , 1.        , 1.        ],\n",
              "         [1.        , 1.        , 1.        ],\n",
              "         [1.        , 1.        , 1.        ],\n",
              "         ...,\n",
              "         [1.        , 1.        , 1.        ],\n",
              "         [1.        , 1.        , 1.        ],\n",
              "         [1.        , 1.        , 1.        ]],\n",
              "\n",
              "        [[1.        , 1.        , 1.        ],\n",
              "         [0.99607843, 0.99607843, 0.99607843],\n",
              "         [0.99607843, 0.99607843, 0.99607843],\n",
              "         ...,\n",
              "         [0.99607843, 0.99607843, 0.99607843],\n",
              "         [0.99607843, 0.99607843, 0.99607843],\n",
              "         [0.99607843, 0.99607843, 0.99607843]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.44313726, 0.47058824, 0.4392157 ],\n",
              "         [0.43529412, 0.4627451 , 0.43529412],\n",
              "         [0.4117647 , 0.4392157 , 0.41568628],\n",
              "         ...,\n",
              "         [0.28235295, 0.31764707, 0.3137255 ],\n",
              "         [0.28235295, 0.3137255 , 0.30980393],\n",
              "         [0.28235295, 0.3137255 , 0.30980393]],\n",
              "\n",
              "        [[0.43529412, 0.4627451 , 0.43137255],\n",
              "         [0.40784314, 0.43529412, 0.40784314],\n",
              "         [0.3882353 , 0.41568628, 0.38431373],\n",
              "         ...,\n",
              "         [0.26666668, 0.29411766, 0.28627452],\n",
              "         [0.27450982, 0.29803923, 0.29411766],\n",
              "         [0.30588236, 0.32941177, 0.32156864]],\n",
              "\n",
              "        [[0.41568628, 0.44313726, 0.4117647 ],\n",
              "         [0.3882353 , 0.41568628, 0.38431373],\n",
              "         [0.37254903, 0.4       , 0.36862746],\n",
              "         ...,\n",
              "         [0.30588236, 0.33333334, 0.3254902 ],\n",
              "         [0.30980393, 0.33333334, 0.3254902 ],\n",
              "         [0.3137255 , 0.3372549 , 0.32941177]]],\n",
              "\n",
              "\n",
              "       ...,\n",
              "\n",
              "\n",
              "       [[[0.13725491, 0.69803923, 0.92156863],\n",
              "         [0.15686275, 0.6901961 , 0.9372549 ],\n",
              "         [0.16470589, 0.6901961 , 0.94509804],\n",
              "         ...,\n",
              "         [0.3882353 , 0.69411767, 0.85882354],\n",
              "         [0.30980393, 0.5764706 , 0.77254903],\n",
              "         [0.34901962, 0.5803922 , 0.7411765 ]],\n",
              "\n",
              "        [[0.22352941, 0.7137255 , 0.91764706],\n",
              "         [0.17254902, 0.72156864, 0.98039216],\n",
              "         [0.19607843, 0.7176471 , 0.9411765 ],\n",
              "         ...,\n",
              "         [0.6117647 , 0.7137255 , 0.78431374],\n",
              "         [0.5529412 , 0.69411767, 0.80784315],\n",
              "         [0.45490196, 0.58431375, 0.6862745 ]],\n",
              "\n",
              "        [[0.38431373, 0.77254903, 0.92941177],\n",
              "         [0.2509804 , 0.7411765 , 0.9882353 ],\n",
              "         [0.27058825, 0.7529412 , 0.9607843 ],\n",
              "         ...,\n",
              "         [0.7372549 , 0.7647059 , 0.80784315],\n",
              "         [0.46666667, 0.5294118 , 0.5764706 ],\n",
              "         [0.23921569, 0.30980393, 0.3529412 ]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.28627452, 0.30980393, 0.3019608 ],\n",
              "         [0.20784314, 0.24705882, 0.26666668],\n",
              "         [0.21176471, 0.26666668, 0.3137255 ],\n",
              "         ...,\n",
              "         [0.06666667, 0.15686275, 0.2509804 ],\n",
              "         [0.08235294, 0.14117648, 0.2       ],\n",
              "         [0.12941177, 0.1882353 , 0.19215687]],\n",
              "\n",
              "        [[0.23921569, 0.26666668, 0.29411766],\n",
              "         [0.21568628, 0.27450982, 0.3372549 ],\n",
              "         [0.22352941, 0.30980393, 0.40392157],\n",
              "         ...,\n",
              "         [0.09411765, 0.1882353 , 0.28235295],\n",
              "         [0.06666667, 0.13725491, 0.20784314],\n",
              "         [0.02745098, 0.09019608, 0.1254902 ]],\n",
              "\n",
              "        [[0.17254902, 0.21960784, 0.28627452],\n",
              "         [0.18039216, 0.25882354, 0.34509805],\n",
              "         [0.19215687, 0.3019608 , 0.4117647 ],\n",
              "         ...,\n",
              "         [0.10588235, 0.20392157, 0.3019608 ],\n",
              "         [0.08235294, 0.16862746, 0.25882354],\n",
              "         [0.04705882, 0.12156863, 0.19607843]]],\n",
              "\n",
              "\n",
              "       [[[0.7411765 , 0.827451  , 0.9411765 ],\n",
              "         [0.7294118 , 0.8156863 , 0.9254902 ],\n",
              "         [0.7254902 , 0.8117647 , 0.92156863],\n",
              "         ...,\n",
              "         [0.6862745 , 0.7647059 , 0.8784314 ],\n",
              "         [0.6745098 , 0.7607843 , 0.87058824],\n",
              "         [0.6627451 , 0.7607843 , 0.8627451 ]],\n",
              "\n",
              "        [[0.7607843 , 0.8235294 , 0.9372549 ],\n",
              "         [0.7490196 , 0.8117647 , 0.9254902 ],\n",
              "         [0.74509805, 0.80784315, 0.92156863],\n",
              "         ...,\n",
              "         [0.6784314 , 0.7529412 , 0.8627451 ],\n",
              "         [0.67058825, 0.7490196 , 0.85490197],\n",
              "         [0.654902  , 0.74509805, 0.84705883]],\n",
              "\n",
              "        [[0.8156863 , 0.85882354, 0.95686275],\n",
              "         [0.8039216 , 0.84705883, 0.9411765 ],\n",
              "         [0.8       , 0.84313726, 0.9372549 ],\n",
              "         ...,\n",
              "         [0.6862745 , 0.7490196 , 0.8509804 ],\n",
              "         [0.6745098 , 0.74509805, 0.84705883],\n",
              "         [0.6627451 , 0.7490196 , 0.84313726]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.8117647 , 0.78039217, 0.70980394],\n",
              "         [0.79607844, 0.7647059 , 0.6862745 ],\n",
              "         [0.79607844, 0.76862746, 0.6784314 ],\n",
              "         ...,\n",
              "         [0.5294118 , 0.5176471 , 0.49803922],\n",
              "         [0.63529414, 0.61960787, 0.5882353 ],\n",
              "         [0.65882355, 0.6392157 , 0.5921569 ]],\n",
              "\n",
              "        [[0.7764706 , 0.74509805, 0.6666667 ],\n",
              "         [0.7411765 , 0.70980394, 0.62352943],\n",
              "         [0.7058824 , 0.6745098 , 0.5764706 ],\n",
              "         ...,\n",
              "         [0.69803923, 0.67058825, 0.627451  ],\n",
              "         [0.6862745 , 0.6627451 , 0.6117647 ],\n",
              "         [0.6862745 , 0.6627451 , 0.6039216 ]],\n",
              "\n",
              "        [[0.7764706 , 0.7411765 , 0.6784314 ],\n",
              "         [0.7411765 , 0.70980394, 0.63529414],\n",
              "         [0.69803923, 0.6666667 , 0.58431375],\n",
              "         ...,\n",
              "         [0.7647059 , 0.72156864, 0.6627451 ],\n",
              "         [0.76862746, 0.7411765 , 0.67058825],\n",
              "         [0.7647059 , 0.74509805, 0.67058825]]],\n",
              "\n",
              "\n",
              "       [[[0.8980392 , 0.8980392 , 0.9372549 ],\n",
              "         [0.9254902 , 0.92941177, 0.96862745],\n",
              "         [0.91764706, 0.9254902 , 0.96862745],\n",
              "         ...,\n",
              "         [0.8509804 , 0.85882354, 0.9137255 ],\n",
              "         [0.8666667 , 0.8745098 , 0.91764706],\n",
              "         [0.87058824, 0.8745098 , 0.9137255 ]],\n",
              "\n",
              "        [[0.87058824, 0.8666667 , 0.8980392 ],\n",
              "         [0.9372549 , 0.9372549 , 0.9764706 ],\n",
              "         [0.9137255 , 0.91764706, 0.9647059 ],\n",
              "         ...,\n",
              "         [0.8745098 , 0.8745098 , 0.9254902 ],\n",
              "         [0.8901961 , 0.89411765, 0.93333334],\n",
              "         [0.8235294 , 0.827451  , 0.8627451 ]],\n",
              "\n",
              "        [[0.8352941 , 0.80784315, 0.827451  ],\n",
              "         [0.91764706, 0.9098039 , 0.9372549 ],\n",
              "         [0.90588236, 0.9137255 , 0.95686275],\n",
              "         ...,\n",
              "         [0.8627451 , 0.8627451 , 0.9098039 ],\n",
              "         [0.8627451 , 0.85882354, 0.9098039 ],\n",
              "         [0.7921569 , 0.79607844, 0.84313726]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.5882353 , 0.56078434, 0.5294118 ],\n",
              "         [0.54901963, 0.5294118 , 0.49803922],\n",
              "         [0.5176471 , 0.49803922, 0.47058824],\n",
              "         ...,\n",
              "         [0.8784314 , 0.87058824, 0.85490197],\n",
              "         [0.9019608 , 0.89411765, 0.88235295],\n",
              "         [0.94509804, 0.94509804, 0.93333334]],\n",
              "\n",
              "        [[0.5372549 , 0.5176471 , 0.49411765],\n",
              "         [0.50980395, 0.49803922, 0.47058824],\n",
              "         [0.49019608, 0.4745098 , 0.4509804 ],\n",
              "         ...,\n",
              "         [0.70980394, 0.7058824 , 0.69803923],\n",
              "         [0.7921569 , 0.7882353 , 0.7764706 ],\n",
              "         [0.83137256, 0.827451  , 0.8117647 ]],\n",
              "\n",
              "        [[0.47843137, 0.46666667, 0.44705883],\n",
              "         [0.4627451 , 0.45490196, 0.43137255],\n",
              "         [0.47058824, 0.45490196, 0.43529412],\n",
              "         ...,\n",
              "         [0.7019608 , 0.69411767, 0.6784314 ],\n",
              "         [0.6431373 , 0.6431373 , 0.63529414],\n",
              "         [0.6392157 , 0.6392157 , 0.6313726 ]]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Normalize the pixel values to [0, 1]\n",
        "# From documentation, I found that the pixel values in each image range from 0 to 255.\n",
        "# Inorder to scale the values, I divided the x_train and x_test, which represent the pixels of the images by 255 inorder to scale the pixel values to the range of [0,1].\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255\n",
        "x_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWDW1ZLySz7K",
        "outputId": "ff2c239c-8bc6-4343-9b5c-133cef970006"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# One-hot encode the labels\n",
        "# Using to_categorical function, we can apply one hot encoding inorder to convert the class labels into binary vectors.\n",
        "# This is done on the Images class data represented by y_train and y_test. Since we have 10 classes we expect 10 columns where\n",
        "# a 1 is present if that are apart of that class.\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4y3zao91ndZB"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and validation sets\n",
        "# Using the train validation split, we want to use the training data as well as the validation set inorder to train and then evaluate the models performance when training to prevent overfitting.\n",
        "# 20 percent of the data will be used for validation, illustrated by test_size=0.2, with a fixed random seed=42.\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiFJsxS5HPHW"
      },
      "source": [
        "VGG16 (Visual Geometry Group 16) is a deep convolutional neural network architecture that was developed by the Visual Geometry Group at the University of Oxford. It was proposed by researchers Karen Simonyan and Andrew Zisserman in their paper titled \"Very Deep Convolutional Networks for Large-Scale Image Recognition,\" which was presented at the International Conference on Learning Representations (ICLR) in 2015.\n",
        "\n",
        "The VGG16 architecture gained significant popularity for its simplicity and effectiveness in image classification tasks. It was one of the pioneering models that demonstrated the power of deeper neural networks for visual recognition tasks.\n",
        "\n",
        "Key characteristics of the VGG16 architecture:\n",
        "\n",
        "1. Architecture: VGG16 consists of a total of 16 layers, hence the name \"16.\" These layers are stacked one after another, forming a deep neural network.\n",
        "\n",
        "2. Convolutional Layers: The main building blocks of VGG16 are the convolutional layers. It primarily uses 3x3 convolutional filters throughout the network, which allows it to capture local features effectively.\n",
        "\n",
        "3. Max Pooling: After each set of convolutional layers, VGG16 applies max-pooling layers with 2x2 filters and stride 2, which halves the spatial dimensions (width and height) of the feature maps and reduces the number of parameters.\n",
        "\n",
        "4. Fully Connected Layers: Towards the end of the network, VGG16 has fully connected layers that act as a classifier to make predictions based on the learned features.\n",
        "\n",
        "5. Activation Function: The network uses the Rectified Linear Unit (ReLU) activation function for all hidden layers, which helps with faster convergence during training.\n",
        "\n",
        "6. Number of Filters: The number of filters in each convolutional layer is relatively small compared to more recent architectures like ResNet or InceptionNet. However, stacking multiple layers allows VGG16 to learn complex hierarchical features.\n",
        "\n",
        "7. Output Layer: The output layer consists of 1000 units, corresponding to 1000 ImageNet classes. VGG16 was originally trained on the large-scale ImageNet dataset, which contains millions of images from 1000 different classes.\n",
        "\n",
        "VGG16 was instrumental in showing that increasing the depth of a neural network can significantly improve its performance on image recognition tasks. However, the main drawback of VGG16 is its high number of parameters, making it computationally expensive and memory-intensive to train. Despite this limitation, VGG16 remains an essential benchmark architecture and has paved the way for even deeper and more efficient models in the field of computer vision, such as ResNet, DenseNet, and EfficientNet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJw9E1D9Q3tQ"
      },
      "source": [
        "Here are your tasks:\n",
        "\n",
        "1. Load [VGG16](https://keras.io/api/applications/vgg/#vgg16-function) as a base model. Make sure to exclude the top layer.\n",
        "2. Freeze all the layers in the base model. We'll be using these weights as a feature extraction layer to forward to layers that are trainable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bncm8oTonijm",
        "outputId": "2a7457a6-b5ce-4850-9b40-59728af99c44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Load the pre-trained VGG16 model (excluding the top classifier)\n",
        "# We want to use the base model VGG16 and training it with the ImageNet dataset, setting the weights to 'imagenet' inorder to perform this.\n",
        "# The reason for adding the pretrained weights is for our model to not have to learn from scratch. Since imagenet contains many images that are categorized into a myraid number of classes.\n",
        "# We also need to set our include_top=false so that we get rid of the fully connected layers at the top of the network, and only include the convolutional layers.\n",
        "# Is is done so we can add our own classication layers based on what we are looking for.\n",
        "VGG16_model = VGG16(weights='imagenet', include_top=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pCQXH4bwS6h5"
      },
      "outputs": [],
      "source": [
        "# Freeze the layers in the base model\n",
        "# By setting the VGG16_model.trainable=False we want to prevent the pre trained convolutional layers from being updated many times. The\n",
        "# reason for this is that the VGG16_model is already being super strong in image training due to having been trained on massive datasets.\n",
        "# This  not only save time but prevent overfitting. The focus will be on only updating the weights of the dense layers that we implimented.\n",
        "VGG16_model.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAnyLR6btWqd"
      },
      "source": [
        "Now, we'll add some trainable layers to the base model.\n",
        "\n",
        "1. Using the base model, add a [GlobalAveragePooling2D](https://keras.io/api/layers/pooling_layers/global_average_pooling2d/) layer, followed by a [Dense](https://keras.io/api/layers/core_layers/dense/) layer of length 256 with ReLU activation. Finally, add a classification layer with 10 units, corresponding to the 10 CIFAR-10 classes, with softmax activation.\n",
        "2. Create a Keras [Model](https://keras.io/api/models/model/) that takes in approproate inputs and outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GiD0CiSvTCRG"
      },
      "outputs": [],
      "source": [
        "# Add a global average pooling layer\n",
        "# By using VGG16_model.output we obtain the the feature maps of the last convolutional layer that represents very detailed information about certain patterns of the input data.\n",
        "# This was done through many convolutional layers and pooling to obtain the VGG16_model.output.\n",
        "# Using the GlobalAveragePooling2D() we can pass the average of the feature map foe each channel as a scalar to reduce the feature map into a 1D frame.\n",
        "# We then get a vector of averages to indicate the strength of a specific feature acrros the whole image, The higher the activation value for that feature map the better it can detect that feature.\n",
        "# For example a feature map illustrating dog ears would have a high activation value because dog ears are strongly detected in that feature.\n",
        "model = GlobalAveragePooling2D()(VGG16_model.output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fKbX1RK4TCB9"
      },
      "outputs": [],
      "source": [
        "# Add a fully connected layer with 256 units and ReLU activation\n",
        "# The 1D model is then placed into a Dense Layer with 256 neurons.\n",
        "# The activation is set to relu for nonlinearity and to get rid of 0 values.\n",
        "# This allows for the highest activiation to be looked at more by focusing on the stronger activations and setting the weaker ones to 0.\n",
        "# This will help to classify the image.\n",
        "fully_connected_layer=Dense(256, activation='relu')(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yEYDagVQTB33"
      },
      "outputs": [],
      "source": [
        "# Add the final classification layer with 10 units (for CIFAR-10 classes) and softmax activation\n",
        "# The 256 neurons are then passed into a another dense layer of 10 representing each of the unique classes the image can fall in.\n",
        "# Using softmax as the activation, gives us the porbability from 0 to 1 that the image would fall in one of the classes.\n",
        "output=Dense(10, activation='softmax')(fully_connected_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "A-op7iObo8io"
      },
      "outputs": [],
      "source": [
        "# Create the fine-tuned model\n",
        "# We then fine tune our model with the following adjustments, we want to create the input layer by taking the VGG16_model.input, which includes the input as well as all the convolutional layers, plus\n",
        "# the outputs which include, the output of the final convolutonal layer, applied by the Global Average Pooling, and then the Dense layers (256 and 10). This is done using the Model function and\n",
        "# apply the applicable inputs and outputs.\n",
        "fine_tuned_model=Model(inputs=VGG16_model.input,outputs=output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjgG_9IMwuLS"
      },
      "source": [
        "With your model complete it's time to train it and assess its performance.\n",
        "\n",
        "1. Compile your model using an appropriate loss function. Feel free to play around with the optimizer, but a good starting optimizer might be Adam with a learning rate of 0.001.\n",
        "2. Fit your model on the training data. Use the validation data to print the accuracy for each epoch. Try training for 10 epochs. Note, training can take a few hours so go ahead and grab a cup of coffee.\n",
        "\n",
        "**Optional**: See if you can implement an [Early Stopping](https://keras.io/api/callbacks/early_stopping/) criteria as a callback function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xTFBXCe6TG5m"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "#Using the model.compile, I am able to configure the model for training using a loss, optimizer, and metric term.\n",
        "#I set the loss term to categorical_crossentropy, since I am dealing with a many different classifications of images not just two with binary. The loss function\n",
        "#tells us how well the models predictions match the actual values we are looking for.\n",
        "#The optimizer that is used is adam to adjust the models weights inorder to minimize the loss. I set to learning rate to a small value of 0.001 to indicate how small the error should be. I also added\n",
        "#an accuracy as metrics inorder to see how the model is doing. The accuracy is the number of correct predictions\n",
        "#divided by total number of predictions. The bases of the compiler is that when the model is used for predicting\n",
        "#a value, the loss function calculates how off the predictions are from the actual values. If the value of the loss function\n",
        "#is not close to 0, the optimizer = 'adam' in this case, will adjust the weights of the model based on the error the model makes.\n",
        "#and it will keep going until there is a loss close to 0 or the best model case.\n",
        "fine_tuned_model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5JnXlGPpkDg",
        "outputId": "a80fe6ce-77d4-4537-c792-9bd06b46435f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m657s\u001b[0m 525ms/step - accuracy: 0.4697 - loss: 1.5202 - val_accuracy: 0.5698 - val_loss: 1.2365\n",
            "Epoch 2/10\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m685s\u001b[0m 527ms/step - accuracy: 0.5790 - loss: 1.1942 - val_accuracy: 0.5853 - val_loss: 1.1898\n",
            "Epoch 3/10\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m641s\u001b[0m 513ms/step - accuracy: 0.6038 - loss: 1.1225 - val_accuracy: 0.5963 - val_loss: 1.1532\n",
            "Epoch 4/10\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m678s\u001b[0m 510ms/step - accuracy: 0.6233 - loss: 1.0740 - val_accuracy: 0.6057 - val_loss: 1.1226\n",
            "Epoch 5/10\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m705s\u001b[0m 528ms/step - accuracy: 0.6360 - loss: 1.0389 - val_accuracy: 0.6097 - val_loss: 1.1193\n",
            "Epoch 6/10\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m640s\u001b[0m 512ms/step - accuracy: 0.6555 - loss: 0.9857 - val_accuracy: 0.6099 - val_loss: 1.1200\n",
            "Epoch 7/10\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m681s\u001b[0m 511ms/step - accuracy: 0.6639 - loss: 0.9578 - val_accuracy: 0.6086 - val_loss: 1.1280\n",
            "Epoch 8/10\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m685s\u001b[0m 514ms/step - accuracy: 0.6840 - loss: 0.9163 - val_accuracy: 0.6189 - val_loss: 1.1060\n",
            "Epoch 9/10\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m680s\u001b[0m 513ms/step - accuracy: 0.6975 - loss: 0.8684 - val_accuracy: 0.6179 - val_loss: 1.1091\n",
            "Epoch 10/10\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m681s\u001b[0m 512ms/step - accuracy: 0.7040 - loss: 0.8488 - val_accuracy: 0.6166 - val_loss: 1.1280\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7eb76aa6f2b0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Train the model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "#Using the fine_tuned_model, we want to use fit so that the model learns from the training data to predict the labels in y_train. The validations set as x_val and y_val are used\n",
        "# to evaluate the model during training. The data will go though the training data set  10 time since our Epoch=10. The EarlyStopping which is used when the validation data does not\n",
        "# inprove, has a patience=5 so that if the validation does not imporve for 5 epochs then the training process will stop. restore_best_weights set to true lets the program know\n",
        "# that once the training is doen the weights go back to the best weights achieved during the training process. Batch size=32 means 32 sets of data are handle in each run, with verbose\n",
        "# of 1 which displays the data below.\n",
        "fine_tuned_model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=10, callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],batch_size=32,verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8-T6EnmypvW"
      },
      "source": [
        "With your model trained, it's time to assess how well it performs on the test data.\n",
        "\n",
        "1. Use your trained model to calculate the accuracy on the test set. Is the model performance better than random?\n",
        "2. Experiment! See if you can tweak your model to improve performance.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auYNYD0JpnaX",
        "outputId": "89564bcf-cc73-41db-a874-735767064231"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 411ms/step - accuracy: 0.6077 - loss: 1.1292\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test set\n",
        "# Using the evaluate function on the x_test and y_test, we found the test_loss which tells us how well the model performed inorder to predict the correct labels.\n",
        "# test_accuracy tells us the amound of correct predictions our model made.\n",
        "# Based on the model I made I have around a 61 percent accuracy, which is not the best and needs to be fine tuned more inorder to achieve a better model performance.\n",
        "# After multiple tests, for some reason I keep getting around the 60th percentile. Since the problem only wants us to use 10 Epoch, I believe if the number of Epochs\n",
        "# increased to 20 but not very large like 100, it can give us better accuracy. Even if we did use an Epoch of 100 EarlyStopping will help us stop whenever the validation\n",
        "# stops improving.\n",
        "test_loss, test_accuracy = fine_tuned_model.evaluate(x_test, y_test, verbose=1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}